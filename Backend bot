const functions = require('@google-cloud/functions-framework');

functions.http('helloHttp', (req, res) => {
  res.send(`Hello ${req.query.name || req.body.name || 'World'}!`);
});
{
  "dependencies": {
    "@google-cloud/functions-framework": "^3.0.0"
  }
}
Pip install transformers datasets torch accelerate
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset

# --- Configuration ---
# You can choose a different base model here. Smaller models are faster to train.
# Examples: "gpt2", "distilgpt2", "microsoft/phi-2" (Phi-2 requires specific setup and permissions sometimes)
MODEL_NAME = "gpt2"
MAX_SEQUENCE_LENGTH = 512 # Increased max_length for better context
TRAIN_BATCH_SIZE = 4 # Adjust based on your GPU memory
NUM_TRAIN_EPOCHS = 3 # Start with 3-5 epochs, adjust based on performance
OUTPUT_DIR = "./my_chatbot_model"
LOGGING_DIR = "./logs"
LEARNING_RATE = 5e-5
WEIGHT_DECAY = 0.01

# --- 1. Load Model and Tokenizer ---
print(f"Loading model and tokenizer for: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# Set pad_token for GPT-like models if not already set
# This is crucial for generation and data collation
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

print("Model and Tokenizer loaded.")

# --- 2. Load and Preprocess Dataset ---
print("Loading dataset...")
# Using Wikitext for demonstration. For a code-generating chatbot,
# a dataset like "the-stack-overflow-dataset" (after agreeing to terms)
# or other code-related text would be more suitable.
# dataset = load_dataset("the-stack-overflow-dataset", data_dir="python", split="train") # Requires manual setup/agreement
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
print("Dataset loaded.")

def preprocess_function(examples):
    # Dynamically find the text column, common in many datasets
    text_column = examples.get("text", examples.get("sentence", None))
    if text_column is None:
        # Fallback for datasets where 'text' or 'sentence' might not be present
        # You might need to inspect your dataset structure (e.g., print(examples.keys()))
        # For wikitext-2-raw-v1, 'text' is the column name.
        raise ValueError("Text column not found in dataset. Check your dataset structure.")

    # Tokenize the text. This will truncate to MAX_SEQUENCE_LENGTH and pad to it.
    return tokenizer(text_column, truncation=True, padding="max_length", max_length=MAX_SEQUENCE_LENGTH)

print("Preprocessing dataset...")
# Apply the preprocessing function in batches for efficiency
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)
print("Dataset preprocessed.")

# --- 3. Data Collator for Causal Language Modeling ---
# This collator handles padding and creating labels for the model
# mlm=False for Causal Language Modeling (next token prediction)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
print("Data Collator created.")

# --- 4. Define Training Arguments ---
print("Defining Training Arguments...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    num_train_epochs=NUM_TRAIN_EPOCHS,
    save_steps=500, # Save checkpoint every 500 steps
    save_total_limit=2, # Keep only the last 2 checkpoints
    logging_dir=LOGGING_DIR,
    report_to="none", # Set to "tensorboard" to visualize training
    learning_rate=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY,
    # evaluation_strategy="epoch", # Uncomment to enable evaluation on a validation set if you load one
    # load_best_model_at_end=True, # Uncomment if evaluation_strategy is enabled
)
print("Training Arguments defined.")

# --- 5. Initialize and Train the Trainer ---
print("Initializing Trainer...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator, # Essential for Causal LM
)
print("Trainer initialized. Starting training...")

# Start the training process
trainer.train()
print("Training complete!")

# --- 6. Save the Fine-Tuned Model and Tokenizer ---
print(f"Saving fine-tuned model and tokenizer to {OUTPUT_DIR}...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("Model and Tokenizer saved.")

# --- 7. Chat Function (Inference) ---
print("\n--- Chatbot Ready! ---")
# To ensure you're using the saved model for inference
# You might want to reload it here if you're running this as a separate script later
# model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)
# tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)

def chat_with_bot(prompt):
    # Tokenize the prompt
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device) # Move to model's device (GPU if available)

    # Generate output
    outputs = model.generate(
        **inputs, # Unpack the dictionary
        max_length=150, # Max length of the generated response (including prompt)
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id, # Stop generation when EOS token is generated
        do_sample=True, # Enable sampling for more creative responses
        top_k=50, # Consider top 50 most likely tokens
        top_p=0.95, # Nucleus sampling: sample from smallest set of tokens whose cumulative probability exceeds 0.95
        temperature=0.7, # Controls randomness (higher = more random)
    )

    # Decode and return the generated text
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Test the Chatbot ---
print("Testing the chatbot...")
prompt1 = "Write Python code for a simple Flask website."
response1 = chat_with_bot(prompt1)
print(f"\nPrompt: {prompt1}")
print(f"Response: {response1}")

prompt2 = "Tell me a short story about a brave knight."
response2 = chat_with_bot(prompt2)
print(f"\nPrompt: {prompt2}")
print(f"Response: {response2}")

# You can also make it interactive
print("\nStart chatting with your bot (type 'quit' to exit):")
while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    bot_response = chat_with_bot(user_input)
    print(f"Bot: {bot_response}")

python chatbot_setup.py
import SwiftUI

// MARK: - API Request/Response Structs
// These must match the JSON structure your Flask API sends and receives
struct ChatRequest: Encodable {
    let prompt: String
}

struct ChatResponse: Decodable {
    let response: String
}

// MARK: - Chat Message Model (for displaying conversation)
struct ChatMessage: Identifiable {
    let id = UUID()
    let text: String
    let isUser: Bool // true if user's message, false if bot's message
}

// MARK: - API Client Function
class ChatAPIClient {
    // !!! IMPORTANT: REPLACE WITH YOUR ACTUAL CLOUD SERVER IP AND PORT !!!
    // Example: "http://192.168.1.100:5000/chat" or "http://your-domain.com:5000/chat"
    private let apiURLString = "http://YOUR_CLOUD_SERVER_IP:5000/chat"

    func sendPromptToChatbot(prompt: String, completion: @escaping (Result<String, Error>) -> Void) {
        guard let url = URL(string: apiURLString) else {
            completion(.failure(NSError(domain: "ChatAPIClient", code: 0, userInfo: [NSLocalizedDescriptionKey: "Invalid API URL"])))
            return
        }

        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        let chatRequest = ChatRequest(prompt: prompt)
        do {
            let jsonData = try JSONEncoder().encode(chatRequest)
            request.httpBody = jsonData
        } catch {
            completion(.failure(error))
            return
        }

        URLSession.shared.dataTask(with: request) { data, response, error in
            // Ensure UI updates happen on the main thread
            DispatchQueue.main.async {
                if let error = error {
                    completion(.failure(error))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    completion(.failure(NSError(domain: "ChatAPIClient", code: 0, userInfo: [NSLocalizedDescriptionKey: "Invalid HTTP response"])))
                    return
                }

                guard httpResponse.statusCode == 200 else {
                    let errorMessage = String(data: data ?? Data(), encoding: .utf8) ?? "Unknown server error"
                    completion(.failure(NSError(domain: "ChatAPIClient", code: httpResponse.statusCode, userInfo: [NSLocalizedDescriptionKey: "Server returned error: \(errorMessage) (Status Code: \(httpResponse.statusCode))"])))))
                    return
                }

                guard let data = data else {
                    completion(.failure(NSError(domain: "ChatAPIClient", code: 0, userInfo: [NSLocalizedDescriptionKey: "No data received from server"])))
                    return
                }

                do {
                    let chatResponse = try JSONDecoder().decode(ChatResponse.self, from: data)
                    completion(.success(chatResponse.response))
                } catch {
                    completion(.failure(error))
                }
            }
        }.resume()
    }
}

// MARK: - SwiftUI View
struct ContentView: View {
    @State private var messages: [ChatMessage] = [] // State for chat history
    @State private var currentPrompt: String = "" // State for current input text
    @State private var isLoading: Bool = false // State for loading indicator
    @State private var errorMessage: String? // State for displaying errors

    private let apiClient = ChatAPIClient() // Instance of our API client

    var body: some View {
        VStack {
            // MARK: Chat History Display
            ScrollView {
                ScrollViewReader { scrollViewProxy in
                    VStack(alignment: .leading, spacing: 10) {
                        ForEach(messages) { message in
                            HStack {
                                if message.isUser {
                                    Spacer()
                                    Text(message.text)
                                        .padding(.horizontal, 12)
                                        .padding(.vertical, 8)
                                        .background(Color.blue)
                                        .foregroundColor(.white)
                                        .cornerRadius(10)
                                } else {
                                    Text(message.text)
                                        .padding(.horizontal, 12)
                                        .padding(.vertical, 8)
                                        .background(Color.gray.opacity(0.2))
                                        .cornerRadius(10)
                                    Spacer()
                                }
                            }
                            .id(message.id) // Assign ID for scrolling
                        }
                    }
                    .padding()
                    .onChange(of: messages.count) { _ in
                        // Scroll to the bottom whenever a new message is added
                        if let lastMessage = messages.last {
                            scrollViewProxy.scrollTo(lastMessage.id, anchor: .bottom)
                        }
                    }
                }
            }

            // MARK: Input Field and Send Button
            HStack {
                TextField("Type your message...", text: $currentPrompt)
                    .textFieldStyle(RoundedBorderTextFieldStyle())
                    .padding(.leading)

                if isLoading {
                    ProgressView()
                        .padding(.trailing)
                } else {
                    Button("Send") {
                        sendMessage()
                    }
                    .padding(.horizontal)
                    .disabled(currentPrompt.isEmpty) // Disable button if input is empty
                }
            }
            .padding(.bottom)

            // MARK: Error Message Display
            if let error = errorMessage {
                Text(error)
                    .foregroundColor(.red)
                    .padding(.horizontal)
            }
        }
        .navigationTitle("My Chatbot") // For NavigationView, if you embed this view
    }

    // MARK: - Send Message Logic
    private func sendMessage() {
        guard !currentPrompt.isEmpty else { return }

        let userMessage = currentPrompt
        messages.append(ChatMessage(text: userMessage, isUser: true))
        currentPrompt = "" // Clear the input field
        errorMessage = nil // Clear any previous error

        isLoading = true // Show loading indicator

        apiClient.sendPromptToChatbot(prompt: userMessage) { result in
            isLoading = false // Hide loading indicator

            switch result {
            case .success(let botResponse):
                messages.append(ChatMessage(text: botResponse, isUser: false))
            case .failure(let error):
                errorMessage = "Error: \(error.localizedDescription)"
                print("API Call Error: \(error)") // Print to Xcode console for debugging
            }
        }
    }
}

// MARK: - Previews (for Xcode canvas)
struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
    }
}
